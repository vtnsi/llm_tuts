{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v3fzWj8DZotF"
   },
   "source": [
    "# Task - Named Entity Recognition (NER)\n",
    "\n",
    "# Introduction\n",
    "\n",
    "NER (Named Entity Recognition) is a fundamental task in Natural Language Processing (NLP). It involves identifying and categorizing named entities in text, like people, organizations, and locations. The main goal of NER is to assess a model's ability to classify named entities in unstructured text into predefined categories.\n",
    "\n",
    "In this tutorial, we will evaluate the Named Entity Recognition (NER) capabilities of the GPT-3.5 Turbo language model. The tutorial uses the CoNLL 2003 dataset, a benchmark for NER tasks, to assess the model's ability to identify and classify named entities such as people, organizations, and locations in text. The notebook includes code to load the dataset, interact with the GPT-3.5 Turbo model via the OpenAI API, and evaluate the model's performance by comparing its predictions to the ground truth annotations in the CoNLL dataset.\n",
    "\n",
    "\n",
    " The CoNLL dataset consists of text data with annotated named entities: Person, Organization, Location, and Miscellaneous.\n",
    "\n",
    "For additional information about the CoNLL benchmark: https://arxiv.org/pdf/cs/0306050v1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qrXPNMPKat-b"
   },
   "source": [
    "# Step 1 - Install Pre-requisites\n",
    "\n",
    "In step 1, we will load the pre-requisites\n",
    "\n",
    "We need to install the following libraries:\n",
    "- `openai`: For interacting with the OpenAI API to query the LLM.\n",
    "- `python-dotenv`: To manage API keys securely using environment variables.\n",
    "- `datasets`: The datasets library provides easy access to a wide variety of datasets commonly used for natural language processing tasks.\n",
    "- `tqdm`: Adds progress bars to loops, making it easier to monitor & visualize the progress\n",
    "- `rich`: A library to render rich text (for display purposes)\n",
    "- `scikit-learn`: A ML library with implmenetations of algorithms, metrics for classification, regression, clustering tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2j1IBt3EXWBo",
    "outputId": "fd153f9c-9df5-437c-f499-7a902b3c6325"
   },
   "outputs": [],
   "source": [
    "%pip install openai==0.28 python-dotenv datasets tqdm rich scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will import the necessary libraries that will be used for various activities such as data processing, API interaction, and environment management tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rK5VttK0Zvpe"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import random\n",
    "from dotenv import load_dotenv\n",
    "from rich import print as rprint\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aOEp2oyTa2VP"
   },
   "source": [
    "# Step 2 - Load LLM\n",
    "\n",
    "Next, to access the GPT3.5 Turbo model via API, we establish a connection using an API key.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_LIdQaqIZ0a6"
   },
   "outputs": [],
   "source": [
    "# Load API key from environment file\n",
    "load_dotenv(dotenv_path=\"../apikey.env.txt\")  # replace the \"file path\" with the location of your API key file\n",
    "\n",
    "APIKEY = os.getenv(\"APIKEY\")\n",
    "\n",
    "openai.api_key = APIKEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oF2VqaG_aD4i"
   },
   "source": [
    "\n",
    "A function to interact with the GPT3.5 turbo model and extract NER tokens from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lFtqh3juZ4eQ"
   },
   "outputs": [],
   "source": [
    "def GetModelResponse(system_content, user_content):\n",
    "    system = {'role': 'system', 'content': system_content}\n",
    "    user = {'role': 'user', 'content': user_content}\n",
    "\n",
    "\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        temperature=1.0,\n",
    "        messages=[system, user],\n",
    "        max_tokens=2000\n",
    "    )\n",
    "\n",
    "    content = response.choices[0].message.content\n",
    "    return content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rg5CX5dVaVJH"
   },
   "source": [
    "# Step 3 - Load test dataset\n",
    "\n",
    "The CoNLL dataset consists of articles with annotated entities like person, organization, location and miscellaneous.\n",
    "\n",
    "Next, we will download and load the CoNLL 2003 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z0z_gcUfaYGl",
    "outputId": "7e68491d-20da-474b-fa58-dd954623baa3"
   },
   "outputs": [],
   "source": [
    "# Download the CoNLL dataset\n",
    "conll_data = load_dataset(\"conll2003\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UQ-Hax0yaeKZ"
   },
   "source": [
    "**Exploring the CoNLL dataset**\n",
    "\n",
    "The CoNLL dataset consists of three parts: a training set, a validation set, and a test set.\n",
    "\n",
    "We will use the test dataset to perform NER evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qJdLXmXOahji",
    "outputId": "6fb517d6-ea49-4553-9611-e26ba3e9ab4e"
   },
   "outputs": [],
   "source": [
    "# Display basic statistics about the dataset\n",
    "train_data = conll_data['train']\n",
    "test_data = conll_data['test']\n",
    "valid_data = conll_data['validation']\n",
    "\n",
    "print(f\"Training set size: {len(train_data)}\")\n",
    "print(f\"Validation set size: {len(valid_data)}\")\n",
    "print(f\"Test set size: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4gUFVWhybBtM"
   },
   "source": [
    "**The test set consists of 3453 instances**.\n",
    "\n",
    "As a tester, you have the option to select the number of instances (out of 3453) on which the LLM will be evaluated. **If no options are provided, the script will default to evaluating 25 instances**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "2R4VYJKwbc0w",
    "outputId": "c28c6fba-1da8-4ce5-f07c-167aaf013907"
   },
   "outputs": [],
   "source": [
    "# Ask user for the number of test instances to evaluate\n",
    "num_test_instances = input(\"Enter the number of test instances to evaluate (default 25): \")\n",
    "num_test_instances = int(num_test_instances) if num_test_instances else 25\n",
    "\n",
    "# Select the random test instances\n",
    "random.seed(12)\n",
    "test_instances = random.sample(list(test_data), num_test_instances)\n",
    "\n",
    "\n",
    "rprint(f\"[blue]Number of test instances: {len(test_instances)}[/blue]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pWSJNnrFhTGJ"
   },
   "source": [
    "# Step 4 - Prompt Construction\n",
    "\n",
    "We will construct the prompt to instruct the model to identify and classify entities from the input text. Then, we will evaluate the model performance by comparing the predicted label with the ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eTLHOppQiglF"
   },
   "outputs": [],
   "source": [
    "# Map numeric tags to string labels for CoNLL dataset\n",
    "label_mapping = {\n",
    "    0: 'O',\n",
    "    1: 'B-PER',\n",
    "    2: 'I-PER',\n",
    "    3: 'B-LOC',\n",
    "    4: 'I-LOC',\n",
    "    5: 'B-ORG',\n",
    "    6: 'I-ORG',\n",
    "    7: 'B-MISC',\n",
    "    8: 'I-MISC'\n",
    "}\n",
    "\n",
    "\n",
    "# Function to extract NER from the model\n",
    "def get_ner(tokens):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant proficient in Named entity Recogintion tasks. The goal is to identify and classify named entities (such as persons, locations, organizations, and miscellaneous entities) in a given list of tokens. The output should be a dictionary where the keys are the tokens and the values are the corresponding entity types, using the following entity tag set: ['O', 'B-PER', 'I-PER', 'B-LOC', 'I-LOC', 'B-ORG', 'I-ORG', 'B-MISC', 'I-MISC'].\"},\n",
    "            {\"role\": \"user\", \"content\": f\" For example, given the tokens ['John', 'Smith', 'lives,' 'in', 'New', 'York', 'City'], the output should be: {{'John': 'B-PER', 'Smith': 'I-PER', 'lives': 'O', 'in': 'O', 'New': 'B-LOC', 'York': 'I-LOC', 'City': 'I-LOC'}}. Now, please provide the named entity tags for the following list of tokens: {tokens}\"}\n",
    "        ]\n",
    "    )\n",
    "    ner_results = response.choices[0].message['content']\n",
    "\n",
    "    try:\n",
    "        ner_dict = eval(ner_results)\n",
    "        if isinstance(ner_dict, dict):\n",
    "            return ner_dict\n",
    "        else:\n",
    "            return {}\n",
    "    except ValueError:\n",
    "        return {}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K-QcZc08sHnE"
   },
   "source": [
    "Next, we'll evaluate the LLM's performance by comparing its predicted NER tags against the ground truth labels.\n",
    "\n",
    "Correctly predicted tokens will be highlighted in green, while mismatches will be displayed in orange."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "pUwwyP7-jjhg",
    "outputId": "558aef92-385b-45d8-dfc8-75fb1be2be35"
   },
   "outputs": [],
   "source": [
    "# Compare LLM predictions with ground truth\n",
    "predictions = []\n",
    "ground_truths = []\n",
    "\n",
    "for instance in test_instances:\n",
    "    tokens = instance['tokens']\n",
    "    ground_truth = {tokens[i]: label_mapping[instance['ner_tags'][i]] for i in range(len(tokens))}\n",
    "    prediction = get_ner(tokens)\n",
    "\n",
    "    print(tokens)\n",
    "    print(ground_truth)\n",
    "    print(prediction)\n",
    "\n",
    "    predictions.append(prediction)\n",
    "    ground_truths.append(ground_truth)\n",
    "\n",
    "    display_text = []\n",
    "    for token in tokens:\n",
    "        pred_tag = prediction.get(token, 'O')\n",
    "        gt_tag = ground_truth[token]\n",
    "        if pred_tag == gt_tag:\n",
    "            display_text.append(f\"<span style='color: green;'>{token}</span>\")\n",
    "        else:\n",
    "            display_text.append(f\"<span style='color: orange;'>{token}</span>\")\n",
    "\n",
    "    display(HTML(f\"<b>Input Text:</b> {' '.join(display_text)}\"))\n",
    "    display(HTML(f\"<b>Ground Truth:</b> {ground_truth}\"))\n",
    "    display(HTML(f\"<b>Prediction:</b> {prediction}\"))\n",
    "    display(HTML(\"<hr>\"))\n",
    "\n",
    "# Function to convert NER tags to a comparable format\n",
    "def convert_to_comparable_format(ner_dict):\n",
    "    return [(word, tag) for word, tag in ner_dict.items()]\n",
    "\n",
    "# Calculate precision, recall, and F1-score\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "for gt, pred in zip(ground_truths, predictions):\n",
    "    gt_converted = convert_to_comparable_format(gt)\n",
    "    pred_converted = convert_to_comparable_format(pred)\n",
    "\n",
    "    for word, tag in gt_converted:\n",
    "        y_true.append(tag)\n",
    "        y_pred.append(pred.get(word, 'O'))  # 'O' for non-entity\n",
    "\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
    "\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
